# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for skywalking.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

serviceAccounts:
  oap:
    # By default, create SkyWalking's ServiceAccount. If set to false, you also need to change `serviceAccounts.oap.name` value to a custom ServiceAccount name.
    create: true
    name: ""

imagePullSecrets: []

initContainer:
  image: busybox
  tag: '1.30'

oap:
  name: oap
  image:
    repository: apache/skywalking-oap-server
    tag: 10.0.1  # Must be set explicitly
    pullPolicy: IfNotPresent
  storageType: elasticsearch
  ports:
    # add more ports here if you need, for example
    # zabbix: 10051
    grpc: 11800
    rest: 12800
    prometheus-port: 1234
  replicas: 1
  service:
    type: ClusterIP
#  javaOpts: -Xmx4g -Xms4g
  antiAffinity: "soft"
  nodeAffinity: {}
  nodeSelector: {}
  tolerations: []
  resources: {}
  resourcesPreset: medium
  livenessProbe:
    tcpSocket:
      port: 12800
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3
  startupProbe:
  # Time to boot the application is set to:
  # 9 (failureThreshold) * 10 (periodSeconds) = 90 seconds in this case.
    tcpSocket:
      port: 12800
    initialDelaySeconds: 60
    failureThreshold: 3
    periodSeconds: 30
  readinessProbe:
    tcpSocket:
      port: 12800
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3
  # podAnnotations:
  #   example: oap-foo
  securityContext: {}
    # runAsUser: 1000
    # runAsGroup: 1000
    # fsGroup: 1000
  env:
    SW_CORE_RECORD_DATA_TTL: 4
    SW_CORE_METRICS_DATA_TTL: 2
    SW_OAL_ENGINE_DEBUG: Y
    SW_CORE_REST_REST_MAX_THREADS: 800
    SW_CORE_REST_QUEUE_SIZE: 800
    SW_TELEMETRY: prometheus
    SW_OTEL_RECEIVER: default
    SW_OTEL_RECEIVER_ENABLED_OTEL_METRICS_RULES: oap

    # more env, please refer to https://hub.docker.com/r/apache/skywalking-oap-server
    # or https://github.com/apache/skywalking-docker/blob/master/6/6.4/oap/README.md#sw_telemetry

  # Allows you to add any config files in /skywalking/config
  # such as log4j2.xml, oal/core.oal, etc.
  config:
       trace-sampling-policy-settings.yml: |
         default:
           rate: 10000
           duration: -1
    # metadata-service-mapping.yaml: |
    #   serviceName: e2e::${LABELS."service.istio.io/canonical-name"}
    #   serviceInstanceName: ${NAME}
       oal:
         core.oal: |
           // Service scope metrics
           service_resp_time = from(Service.latency).longAvg();
           service_sla = from(Service.*).percent(status == true);
           service_cpm = from(Service.*).cpm();
           service_percentile = from(Service.latency).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
           service_apdex = from(Service.latency).apdex(name, status);
           // service_mq_consume_count = from(Service.*).filter(type == RequestType.MQ).count();
           // service_mq_consume_latency = from((str->long)Service.tag["transmission.latency"]).filter(type == RequestType.MQ).filter(tag["transmission.latency"] != null).longAvg();
            
           // Service relation scope metrics for topology
           // service_relation_client_cpm = from(ServiceRelation.*).filter(detectPoint == DetectPoint.CLIENT).cpm();
           service_relation_server_cpm = from(ServiceRelation.*).filter(detectPoint == DetectPoint.SERVER).cpm();
           // service_relation_client_call_sla = from(ServiceRelation.*).filter(detectPoint == DetectPoint.CLIENT).percent(status == true);
           service_relation_server_call_sla = from(ServiceRelation.*).filter(detectPoint == DetectPoint.SERVER).percent(status == true);
           // service_relation_client_resp_time = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.CLIENT).longAvg();
           service_relation_server_resp_time = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.SERVER).longAvg();
           // service_relation_client_percentile = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.CLIENT).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
           service_relation_server_percentile = from(ServiceRelation.latency).filter(detectPoint == DetectPoint.SERVER).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
            
           // Service Instance relation scope metrics for topology
           // service_instance_relation_client_cpm = from(ServiceInstanceRelation.*).filter(detectPoint == DetectPoint.CLIENT).cpm();
           // service_instance_relation_server_cpm = from(ServiceInstanceRelation.*).filter(detectPoint == DetectPoint.SERVER).cpm();
           // service_instance_relation_client_call_sla = from(ServiceInstanceRelation.*).filter(detectPoint == DetectPoint.CLIENT).percent(status == true);
           // service_instance_relation_server_call_sla = from(ServiceInstanceRelation.*).filter(detectPoint == DetectPoint.SERVER).percent(status == true);
           // service_instance_relation_client_resp_time = from(ServiceInstanceRelation.latency).filter(detectPoint == DetectPoint.CLIENT).longAvg();
           // service_instance_relation_server_resp_time = from(ServiceInstanceRelation.latency).filter(detectPoint == DetectPoint.SERVER).longAvg();
           // service_instance_relation_client_percentile = from(ServiceInstanceRelation.latency).filter(detectPoint == DetectPoint.CLIENT).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
           // service_instance_relation_server_percentile = from(ServiceInstanceRelation.latency).filter(detectPoint == DetectPoint.SERVER).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
            
           // Service Instance Scope metrics
           // service_instance_sla = from(ServiceInstance.*).percent(status == true);
           service_instance_resp_time = from(ServiceInstance.latency).longAvg();
           // service_instance_cpm = from(ServiceInstance.*).cpm();
            
           // Endpoint scope metrics
           endpoint_cpm = from(Endpoint.*).cpm();
           endpoint_resp_time = from(Endpoint.latency).longAvg();
           //endpoint_sla = from(Endpoint.*).percent(status == true);
           endpoint_sla = from(Endpoint.*).percent(httpResponseStatusCode < 300);
           endpoint_percentile = from(Endpoint.latency).percentile2(10); // Multiple values including p50, p75, p90, p95, p99
           endpoint_mq_consume_latency = from((str->long)Endpoint.tag["transmission.latency"]).filter(type == RequestType.MQ).filter(tag["transmission.latency"] != null).longAvg();
           // 统计每个服务响应码在[500,599]之间的百分比
           //endpoint_sla5xx = from(Endpoint.*).percent(httpResponseStatusCode in [500,501,502,503,504,505,506,507]); // New
           endpoint_sla5xx = from(Endpoint.*).percent(httpResponseStatusCode > 499); // New
           // endpoint_sla4xx = from(Endpoint.*).percent(or(httpResponseStatusCode > 499, httpResponseStatusCode < 400)); // New
           // 统计每个服务响应码在[400,499]之间的百分比
           endpoint_sla4xx = from(Endpoint.*).percent(httpResponseStatusCode in [400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417]); // New
           //endpoint_sla4xx = from(Endpoint.*).percent(httpResponseStatusCode > 399 AND httpResponseStatusCode < 500); // New
           //endpoint_sla4xx = from(Endpoint.*).cpm(); // New
           endpoint_sla3xx = from(Endpoint.*).percent(httpResponseStatusCode in [300,301,302,303,304,307,308]); // New
           
           // Endpoint relation scope metrics
           endpoint_relation_cpm = from(EndpointRelation.*).filter(detectPoint == DetectPoint.SERVER).cpm();
           endpoint_relation_resp_time = from(EndpointRelation.rpcLatency).filter(detectPoint == DetectPoint.SERVER).longAvg();
           endpoint_relation_sla = from(EndpointRelation.*).filter(detectPoint == DetectPoint.SERVER).percent(status == true);
           endpoint_relation_percentile = from(EndpointRelation.rpcLatency).filter(detectPoint == DetectPoint.SERVER).percentile2(10); // Multiple values including p50, p75, p90, p95,p99
            
           database_access_resp_time = from(DatabaseAccess.latency).longAvg();
           database_access_sla = from(DatabaseAccess.*).percent(status == true);
           database_access_cpm = from(DatabaseAccess.*).cpm();
           database_access_percentile = from(DatabaseAccess.latency).percentile2(10);
            
           cache_read_resp_time = from(CacheAccess.latency).filter(operation == VirtualCacheOperation.Read).longAvg();
           cache_read_sla = from(CacheAccess.*).filter(operation == VirtualCacheOperation.Read).percent(status == true);
           cache_read_cpm = from(CacheAccess.*).filter(operation == VirtualCacheOperation.Read).cpm();
           cache_read_percentile = from(CacheAccess.latency).filter(operation == VirtualCacheOperation.Read).percentile2(10);
            
           cache_write_resp_time = from(CacheAccess.latency).filter(operation == VirtualCacheOperation.Write).longAvg();
           cache_write_sla = from(CacheAccess.*).filter(operation == VirtualCacheOperation.Write).percent(status == true);
           cache_write_cpm = from(CacheAccess.*).filter(operation == VirtualCacheOperation.Write).cpm();
           cache_write_percentile = from(CacheAccess.latency).filter(operation == VirtualCacheOperation.Write).percentile2(10);
            
           cache_access_resp_time = from(CacheAccess.latency).longAvg();
           cache_access_sla = from(CacheAccess.*).percent(status == true);
           cache_access_cpm = from(CacheAccess.*).cpm();
           cache_access_percentile = from(CacheAccess.latency).percentile2(10);
            
           // mq_service_consume_cpm = from(MQAccess.*).filter(operation == MQOperation.Consume).cpm();
           // mq_service_consume_sla = from(MQAccess.*).filter(operation == MQOperation.Consume).percent(status == true);
           // mq_service_consume_latency = from(MQAccess.transmissionLatency).filter(operation == MQOperation.Consume).longAvg();
           // mq_service_consume_percentile = from(MQAccess.transmissionLatency).filter(operation == MQOperation.Consume).percentile2(10);
           // mq_service_produce_cpm = from(MQAccess.*).filter(operation == MQOperation.Produce).cpm();
           // mq_service_produce_sla = from(MQAccess.*).filter(operation == MQOperation.Produce).percent(status == true);
            
           mq_endpoint_consume_cpm = from(MQEndpointAccess.*).filter(operation == MQOperation.Consume).cpm();
           mq_endpoint_consume_latency = from(MQEndpointAccess.transmissionLatency).filter(operation == MQOperation.Consume).longAvg();
           mq_endpoint_consume_percentile = from(MQEndpointAccess.transmissionLatency).filter(operation == MQOperation.Consume).percentile2(10);
           mq_endpoint_consume_sla = from(MQEndpointAccess.*).filter(operation == MQOperation.Consume).percent(status == true);
           mq_endpoint_produce_cpm = from(MQEndpointAccess.*).filter(operation == MQOperation.Produce).cpm();
           mq_endpoint_produce_sla = from(MQEndpointAccess.*).filter(operation == MQOperation.Produce).percent(status == true);

# log4j2.xml: |
    #   <Configuration status="DEBUG">
    #     <!-- ... -->
    #   </Configuration>
  # When 'dynamicConfig.enabled' set to true, enable oap dynamic configuration through k8s configmap，
  # Note: The default configmap data is empty, please refer to the detailed documentation (https://github.com/apache/skywalking/blob/master/docs/en/setup/backend/dynamic-config.md)
  # Sync period in seconds. Defaults to 60 seconds.
  dynamicConfig:
    enabled: false
    period: 60
    #  agent-analyzer.default.slowDBAccessThreshold: default:200,mongodb:50
    #  alarm.default.alarm-settings: |
    #   rules:
    #     # Rule unique name, must be ended with `_rule`.
    #     service_resp_time_rule:
    #       metrics-name: service_resp_time
    #       op: ">"
    #       threshold: 1000
    #       period: 10
    #       count: 3
    #       silence-period: 5
    #       Response time of service {name} is more than 1000ms in 3 minutes of last 10 minutes.
  persistence:
    enabled: false
ui:
  name: ui
  replicas: 1
  image:
    repository: apache/skywalking-ui
    tag: 10.0.1  # Must be set explicitly
    pullPolicy: IfNotPresent
  # podAnnotations:
  #   example: oap-foo
  nodeAffinity: {}
  nodeSelector: {}
  tolerations: []
  resourcesPreset: medium
  resources: {}
  ingress:
    enabled: true
    annotations:
       kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - skywalking-tw.intranet.local
    # - skywalking.local
    tls: []
    #  - secretName: skywalking-tls
    #    hosts:
    #      - skywalking.local
  service:
    type: ClusterIP
    # clusterIP: None
    externalPort: 80
    internalPort: 8080
    ## nodePort is the port on each node on which this service is exposed when type=NodePort
    ## Default: auto-allocated port if not specified. 30080 is just an example
    ##
    # nodePort: 30080
    ## External IP addresses of service
    ## Default: nil
    ##
    # externalIPs:
    # - 192.168.0.1
    #
    ## LoadBalancer IP if service.type is LoadBalancer
    ## Default: nil
    ##
    # loadBalancerIP: 10.2.2.2
    # Annotation example: setup ssl with aws cert when service.type is LoadBalancer
    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:EXAMPLE_CERT
    annotations: {}
    ## Limit load balancer source ips to list of CIDRs (where available)
    # loadBalancerSourceRanges: []
  securityContext: {}
    # runAsUser: 1000
    # runAsGroup: 1000
    # fsGroup: 1000
  env:

oapInit:
  nodeAffinity: {}
  nodeSelector: {}
  tolerations: []

elasticsearch:
  enabled: false
  config:               # For users of an existing elasticsearch cluster,takes effect when `elasticsearch.enabled` is false
    port:
      http: 9200
    host: elasticsearch # es service on kubernetes or host
    user: "xxx"         # [optional]
    password: "xxx"     # [optional]
  clusterName: "elasticsearch"
  nodeGroup: "master"

  # The service that non master groups will try to connect to when joining the cluster
  # This should be set to clusterName + "-" + nodeGroup for your master group
  masterService: ""

  # Elasticsearch roles that will be applied to this nodeGroup
  # These will be set as environment variables. E.g. node.master=true
  roles:
    master: "true"
    ingest: "true"
    data: "true"

  replicas: 0
  minimumMasterNodes: 2

  esMajorVersion: ""

  # Allows you to add any config files in /usr/share/elasticsearch/config/
  # such as elasticsearch.yml and log4j2.properties
  esConfig: {}
  #  elasticsearch.yml: |
  #    key:
  #      nestedkey: value
  #  log4j2.properties: |
  #    key = value

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs

  podAnnotations: {}
  # iam.amazonaws.com/role: es-cluster

  # additionals labels
  labels: {}

  esJavaOpts: "-Xmx2g -Xms2g"

  resources:
    requests:
      cpu: "200m"
      memory: "2Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"

  initResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
    # requests:
    #   cpu: "25m"
  #   memory: "128Mi"

  sidecarResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
    # requests:
    #   cpu: "25m"
  #   memory: "128Mi"

  networkHost: "0.0.0.0"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "gp3"
    resources:
      requests:
        storage: 100Gi

  rbac:
    create: false
    serviceAccountName: ""

  podSecurityPolicy:
    create: false
    name: ""
    spec:
      privileged: true
      fsGroup:
        rule: RunAsAny
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
        - secret
        - configMap
        - persistentVolumeClaim

  persistence:
    enabled: false
    annotations: {}

  extraVolumes: ""
    # - name: extras
  #   emptyDir: {}

  extraVolumeMounts: ""
    # - name: extras
    #   mountPath: /usr/share/extras
  #   readOnly: true

  extraInitContainers: ""
    # - name: do-something
    #   image: busybox
  #   command: ['do', 'something']

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  # This is the node affinity settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
  nodeAffinity: {}

  # The default is to deploy all pods serially. By setting this to parallel all pods are started at
  # the same time when bootstrapping the cluster
  podManagementPolicy: "Parallel"

  protocol: http
  httpPort: 9200
  transportPort: 9300

  service:
    labels: {}
    labelsHeadless: {}
    type: ClusterIP
    nodePort: ""
    annotations: {}
    httpPortName: http
    transportPortName: transport

  updateStrategy: RollingUpdate

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 0

  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000

  # The following value is deprecated,
  # please use the above podSecurityContext.fsGroup instead
  fsGroup: ""

  securityContext:
    capabilities:
      drop:
        - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  # How long to wait for elasticsearch to stop gracefully
  terminationGracePeriod: 120

  sysctlVmMaxMapCount: 262144

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  # https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

  ## Use an alternate scheduler.
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""

  image: elasticsearch
  imagePullPolicy: IfNotPresent
  imageTag: 7.5.1

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []

  # Enabling this will publically expose your Elasticsearch instance.
  # Only enable this if you have security enabled on your cluster
  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  nameOverride: ""
  fullnameOverride: ""

  # https://github.com/elastic/helm-charts/issues/63
  masterTerminationFix: false

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # postStart:
    #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

  sysctlInitContainer:
    enabled: true

  keystore: []

postgresql:
  enabled: false # Whether to start a demo postgresql deployment, don't use this for production.
  config:
    # The hostname of your own postgresql service, this only takes effect when postgresql.enabled is false.
    host: postgresql.opspilot.svc.cluster.local
  auth:
    username: postgres
    password: "xxxxxx"
    database: skywalking
  containerPorts:
    postgresql: 5432
  primary:
    persistence:
      enabled: false
  readReplicas:
    persistence:
      enabled: false
  resources:
    requests:
      cpu: "200m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
satellite:
  name: satellite
  replicas: 1
  enabled: false
  image:
    repository: skywalking.docker.scarf.sh/apache/skywalking-satellite
    tag: null # Must be set explicitly
    pullPolicy: IfNotPresent
  ports:
    grpc: 11800
    prometheus: 1234
    # Disable the pprof port by default, only enabled it when you need to debug the satellite.
    # pprof: 6060
  service:
    type: ClusterIP
  antiAffinity: "soft"
  nodeAffinity: {}
  nodeSelector: {}
  tolerations: []
  resources: {}
    # limits:
    #   cpu: 4
    #   memory: 8Gi
    # requests:
    #   cpu: 4
    #   memory: 4Gi
  podAnnotations:
    # example: oap-foo
  env:
    # more env, please refer to https://skywalking.apache.org/docs/skywalking-satellite/latest/en/setup/readme/#satellite_configyaml
  # Allows you to add any config files in /skywalking/config.
  config: {}
    # satellite_config.yaml: |
    #   key: val
  securityContext: {}
    # runAsUser: 1000
    # runAsGroup: 1000
    # fsGroup: 1000

nameOverride: ""
fullnameOverride: "skywalking"
